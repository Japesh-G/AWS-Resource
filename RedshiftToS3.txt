import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import *
from awsglue.dynamicframe import DynamicFrame

## @params: [TempDir, JOB_NAME]
args = getResolvedOptions(sys.argv, ['TempDir','JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
## @type: DataSource
## @args: [database = "redshifttos3", table_name = "dev_public_tablename", redshift_tmp_dir = TempDir, transformation_ctx = "datasource0"]
## @return: datasource0
## @inputs: []
datasource0 = glueContext.create_dynamic_frame.from_catalog(database = "redshifttos3", table_name = "dev_public_tablename", redshift_tmp_dir = args["TempDir"], transformation_ctx = "datasource0")
## @type: ApplyMapping
## @args: [mapping = [("year", "int", "year", "int"), ("rating", "double", "rating", "double"), ("rank", "int", "rank", "int"), ("movie_title", "string", "movie_title", "string")], transformation_ctx = "applymapping1"]
## @return: applymapping1
newDF = datasource0.toDF().withColumn("arpitproject", lit("ABC"))
datasource0 = DynamicFrame.fromDF(newDF, glueContext, "datasource0")
## @inputs: [frame = datasource0]
applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [("year", "int", "year", "int"), ("rating", "double", "rating", "double"), ("rank", "int", "rank", "int"), ("movie_title", "string", "movie_title", "string"), ("arpitproject", "string", "arpitproject", "string")], transformation_ctx = "applymapping1")
## @type: DataSink
## @args: [connection_type = "s3", connection_options = {"path": "s3://seriousdata/writting/"}, format = "csv", transformation_ctx = "datasink2"]
## @return: datasink2

## @inputs: [frame = applymapping1]
# newdf=applymapping1.toDF().withColumn("arpitproject",lit("project"))

repartition = applymapping1.repartition(1)
# repartition.toDF().write.fomat("csv").save("s3://seriousdata/writting/arpit.csv")
repartition.toDF().write.mode("overwrite").option("header",True).csv('s3://seriousdata/writting/foldername')
# datasink2 = glueContext.write_dynamic_frame.from_options(frame = repartition, connection_type = "s3", connection_options = {"path": "s3://seriousdata/writting/ar.csv"}, format = "csv", transformation_ctx = "datasink2")
import boto3
client = boto3.client('s3')
#getting all the content/file inside the bucket. 
response = client.list_objects_v2(Bucket="seriousdata")
names = response["Contents"]
#Find out the file which have part-000* in it's Key
particulars = [name['Key'] for name in names if 'part-000' in name['Key']]
#Find out the prefix of part-000* because we want to retain the partitions schema 
location = [particular.split('part-000')[0] for particular in particulars]
#Constrain - copy_object has limit of 5GB.datepartition=20190131
for key,particular in enumerate(particulars):
    client.copy_object(Bucket="seriousdata", CopySource="seriousdata" + "/" + particular, Key=location[key]+"japeshhh.csv")
    client.delete_object(Bucket="seriousdata", Key=particular)

job.commit()
