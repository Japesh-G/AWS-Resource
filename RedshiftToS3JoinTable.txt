import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import *
from awsglue.dynamicframe import DynamicFrame

## @params: [TempDir, JOB_NAME]
args = getResolvedOptions(sys.argv, ['TempDir','JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
## @type: DataSource
## @args: [database = "newdatabaseforjoin", table_name = "dev_public_date", redshift_tmp_dir = TempDir, transformation_ctx = "datasource0"]
## @return: datasource0
## @inputs: []
datasource0 = glueContext.create_dynamic_frame.from_catalog(database = "newdatabaseforjoin", table_name = "dev_public_date", redshift_tmp_dir = args["TempDir"], transformation_ctx = "datasource0")
datasource1 = glueContext.create_dynamic_frame.from_catalog(database = "redshifttos3", table_name = "dev_public_dataaxle", redshift_tmp_dir = args["TempDir"], transformation_ctx = "datasource1")
## @type: ApplyMapping
## @args: [mapping = [("caldate", "date", "caldate", "date"), ("week", "short", "week", "short"), ("month", "string", "month", "string"), ("year", "short", "year", "short"), ("dateid", "short", "dateid", "short"), ("day", "string", "day", "string"), ("holiday", "boolean", "holiday", "boolean"), ("qtr", "string", "qtr", "string")], transformation_ctx = "applymapping1"]
## @return: applymapping1
## @inputs: [frame = datasource0]
# applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [("caldate", "date", "caldate", "date"), ("week", "short", "week", "short"), ("month", "string", "month", "string"), ("year", "short", "year", "short"), ("dateid", "short", "dateid", "short"), ("day", "string", "day", "string"), ("holiday", "boolean", "holiday", "boolean"), ("qtr", "string", "qtr", "string")], transformation_ctx = "applymapping1")
## @type: DataSink
## @args: [connection_type = "s3", connection_options = {"path": "s3://seriousdata/writting/dataaxle"}, format = "csv", transformation_ctx = "datasink2"]
## @return: datasink2
## @inputs: [frame = applymapping1]
datasource0 = datasource0.toDF()
datasource1 = datasource1.toDF()
newDF = datasource1.join(datasource0,['year'],'inner')
datasource = DynamicFrame.fromDF(newDF, glueContext, "newDF")
## @inputs: [frame = datasource0]
# applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [("year", "int", "year", "int"), ("rating", "double", "rating", "double"), ("rank", "int", "rank", "int"), ("movie_title", "string", "movie_title", "string"), ("arpitproject", "string", "arpitproject", "string")], transformation_ctx = "applymapping1")
## @type: DataSink
## @args: [connection_type = "s3", connection_options = {"path": "s3://seriousdata/writting/"}, format = "csv", transformation_ctx = "datasink2"]
## @return: datasink2

## @inputs: [frame = applymapping1]
# newdf=applymapping1.toDF().withColumn("arpitproject",lit("project"))

repartition = datasource.repartition(1)
# repartition.toDF().write.fomat("csv").save("s3://seriousdata/writting/arpit.csv")
repartition.toDF().write.mode("overwrite").option("header",True).csv('s3://seriousdata/writting/dataaxle')
# datasink2 = glueContext.write_dynamic_frame.from_options(frame = repartition, connection_type = "s3", connection_options = {"path": "s3://seriousdata/writting/ar.csv"}, format = "csv", transformation_ctx = "datasink2")
import boto3
client = boto3.client('s3')
#getting all the content/file inside the bucket. 
response = client.list_objects_v2(Bucket="seriousdata")
names = response["Contents"]
#Find out the file which have part-000* in it's Key
particulars = [name['Key'] for name in names if 'part-000' in name['Key']]
#Find out the prefix of part-000* because we want to retain the partitions schema 
location = [particular.split('part-000')[0] for particular in particulars]
#Constrain - copy_object has limit of 5GB.datepartition=20190131
for key,particular in enumerate(particulars):
    client.copy_object(Bucket="seriousdata", CopySource="seriousdata" + "/" + particular, Key=location[key]+"japeshsrpit.csv")
    client.delete_object(Bucket="seriousdata", Key=particular)

job.commit()